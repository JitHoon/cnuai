{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 다층 퍼셉트론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가장 기본적인 단위인 \n",
    "\n",
    "`퍼셉트론` (인공두뇌학 `cybernestic`) \n",
    "\n",
    "-> `다층 퍼셉트론` (결합설 `connectionism`) \n",
    "\n",
    "-> `깊은 인공신경망` (심층학습 `deep learning`) \n",
    "\n",
    "순서로 살펴봄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 사람의 뉴런 (`neuron`)과 `퍼셉트론`\n",
    "< `neuron` : `두뇌의 가장 작은 정보처리 단위` >\n",
    "\n",
    "- 구조 : 세포체 (`연산`), 축삭 (`전송`), 수상돌기 (`수신` (정보를 받음))\n",
    "\n",
    "- 뇌의 정보처리를 컴퓨터 계산 (연산) 능력으로 모방하여 인공 신경망의 기초 단위인 `퍼셉트론`을 고안\n",
    "\n",
    "< `퍼셉트론` : `인공 신경망의 기초 단위` >\n",
    "\n",
    "- 구조 : 절 (`node`), 가중치 (`weight`), 층 (`layer`)\n",
    "\n",
    "- 깊은 인공신경망을 포함한 현대 인공신경망의 토대"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![neuron](./pic/neuron.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![perceptron](./pic/perceptron.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 전방(`forward`), 순환(`recurrent`), 얕은(`shallow`), 깊은(`deep`) 신경망\n",
    "\n",
    "![models](./pic/models.png) \n",
    "\n",
    "- 은닉층 (네트워크)의 구조가 크냐 작냐로 깊이를 구분\n",
    "- 깊은 신경망이 얕은 신경망 보다 가중치가 더 많기 때문에 모델 cap이 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 결정론(`deterministic`), 확률론적 (`stochastic`) 신경망\n",
    "- `결정론` : 모델의 매개변수와 조건에 의해 출력이 완전히 결정되는 신경망\n",
    "\n",
    "- `확률론적` : 고유의 `임의성`을 가지고 매개변수와 조건이 같더라도 다른 출력을 가지는 신경망 (`불확실성을 고려함`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![models2](./pic/models2.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. `퍼셉트론의 구조`\n",
    "- 크게 절 (`node`), 가중치 (`weight`), 층 (`layer`) 로 구성되고 입력, 입출력 연산, 출력 구조에 포함되어있다.\n",
    "\n",
    "1. `입력` : 특징 벡터 x 입력을 node를 통해 받음, 항상 1이 입력되는 `bias node` 포함 (`bias node` : 임게치 `T`의 위치를 원점으로 맞춰주는 역할)\n",
    "\n",
    "\n",
    "2. `입출력 연산 구조` : 입력 노드와 축력 노드를 연결하는 `edge에 weight (가중치 w)를 가짐` (퍼셉트론은 단일 층 구조로 간주)\n",
    "\n",
    "\n",
    "3. `출력` : 한 개의 노드에 의해 수치 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. `퍼셉트론의 동작`\n",
    "- `선형` 연산 -> `비선형` 연산\n",
    "\n",
    "- `선형` 연산 : 입력과 가중치의 `내적` (합과 곱, `내적` : `입력이 가중치와의 방향성이 유사할수록 (유사도가 높을수록) 큰 값이 나옴!`)\n",
    "- `비선형` 연산 : 활성함수 타우를 적용 (`가중치에 따른 1과 -1 출력`)\n",
    "\n",
    "![perceptron3](./pic/perceptron3.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![example](./pic/example.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래는 퍼셉트론 동작중 `임계치의 직선화 및 기하학적 해석`이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![perceptron4](./pic/perceptron4.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 일반적인 분류기의 학습 과정 (그래서 학습은 어떻게?)\n",
    "1. 과업 정의와 분류과정의 수학적 정의 (`가설 설정` (퍼셉트론))\n",
    "\n",
    "2. 해당 분류기의 `목적함수 J(세타) 정의` (손실 함수이므로 클수록 학습이 좋지 않음)\n",
    "\n",
    "3. `J(세타)를 최소화하는 세타를 찾기` 위한 최적화 방법 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Jdefine](./pic/Jdefine.png) \n",
    "![makeJ](./pic/makeJ.png) \n",
    "![findJ](./pic/findJ.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 퍼셉트론의 학습 방법인 `가중치 갱신 규칙`을 활용한 `델타 규칙`을 사용하기 위해서 목적함수를 편미분한 경사도 g가 필요하다.\n",
    "\n",
    "- 이때 로 (학습률 `learning rate`)는 얼만큼 방향을 수정할 지 결정하는 길이(거리, step)이라고 해석하면 된다.\n",
    "\n",
    "![g](./pic/g.png) \n",
    "\n",
    "- 아래는 퍼셉트론의 학습 방법중 하나인 `델타 규칙`이다.\n",
    "\n",
    "![rule](./pic/rule.png) \n",
    "\n",
    "- 예시를 보면 틀린 샘플로 인해 가중치가 갱신된다. 이 가중치들의 합 벡터에 직교하는 직선을 그으면 가중치가 반영된 결정 직선을 만들 수 있다. 결과적인 관점에서 새로운 결정 직선은 틀린 샘플과 직선의 중점을 기준으로 뒤집힌 직선으로 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![deltaex](./pic/deltaex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cf. 퍼셉트론 `하나`는 `선형분류만 가능`하다. 따라서 `다층 퍼셉트론이 필요`하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. `다층 퍼셉트론의 핵심 아이디어 세 가지`\n",
    "1. `은닉층`을 둔다. \n",
    "\n",
    "(`유리하고 새로운 특징 공간`으로 변환 가능)\n",
    "\n",
    "2. `sigmoid 활성함수`를 도입\n",
    "\n",
    "(sigmoid : 시그모이드 함수는 S자형 곡선 또는 시그모이드 곡선을 갖는 수학 함수이, 활성함수 : 비선형적 요소, 내적 : 선형적 요소)\n",
    "\n",
    "(퍼셉트론의 경우 계단 함수가 활성함수)\n",
    "\n",
    "3. `오류 역전파 알고리즘` 사용. (한 층씩 그레디언트를 계산하고 가중치를 갱신)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. `특징 공간 변환`\n",
    "#### 퍼셉트론 2개를 사용한 XOR 문제의 해결\n",
    "![xor](./pic/xor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 퍼셉트론 2개를 `병렬 결합`하면, 아래 그림과 같이 원래 공간을 `새로운 특징 공간으로 변환이 가능`해지고 선형 분리가 가능해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![xor2](./pic/xor2.png)\n",
    "\n",
    "- 이는 사람이 hand-craft features learning (수작업 특징 학습) 을 수행한 것과 유사하다. (표현 학습)\n",
    "\n",
    "- 여기에 다른 퍼셉트론 1개를 `순차 결합` 하면 선형적 풀이가 가능해진다.\n",
    "\n",
    "![xor3](./pic/xor3.png)\n",
    "\n",
    "- 일반화하면 `p 개의 퍼셉트론을 결합하면 p 차원 공간으로 변환이 가능`하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. `대표적인 활성 함수 두 가지`\n",
    "1. `이진 sigmoid`\n",
    "2. `양극 sigmoid`\n",
    "\n",
    "![sigmoid](./pic/sigmoid.png)\n",
    "\n",
    "- 다양한 활성 함수\n",
    "\n",
    "![sigmoid2](./pic/sigmoid2.png)\n",
    "\n",
    "- 소프트플러스, ReLU는 깊은 신경망의 활성 함수\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 깊은 신경망에서 ReLU를 사용하는 이유 : 오류 역전파를 사용하는 과정에서 gradient가 사용되는데 sigmoid의 경우 `오류로 들어온 0의 gradient로 인해 입력까지 오류 역전파가 전달되지 않는 경우가 발생한다.` 이 때문에 오류 역전파에 사용되는 gradient의 값이 무조건 1로 들어오는 ReLU 가 사용됨.\n",
    "\n",
    "![ReLU](./pic/ReLU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. `다층 퍼셉트론의 구조와 은닉층 해석`\n",
    "\n",
    "![deepmatrix](./pic/deepmatrix.png)\n",
    "\n",
    "- n층 구조 : `입력 노드` (d+1) - `은닉 노드` (p, q ...) - `출력 노드` (c)\n",
    "\n",
    "- n층 구조를 `특징 벡터` x를 `출력 벡터` o로 `mapping (사상)하는 함수`로 간주할 수 있음 \n",
    "\n",
    "\n",
    "- 하나의 은닉층은 함수의 근사를 표현한다고 볼 수 있으므로 다층 퍼셉트론은 공간을 변환하는 근사 함수라고 해석이 가능하다. (`범용적 근사 이론 universal approximation theorem`)\n",
    "\n",
    "`mapping 함수` 표현 : $ o = f_L(...f_2(f_1(x)))) $\n",
    "\n",
    "`matrix 연산` 표현 : $ o = t(U^2t_h(U^1x)) $ : 입력(x)과 첫번째 퍼셉트론(U)과의 내적을 확성 함수(t)에 입력\n",
    "\n",
    "- 은닉층은 `특징 추출기`라고도 불린다.\n",
    "\n",
    "- 은닉층은 `입력 공간을 어디서 접을지 지정`하는 것과 같으며 `여러 선형적인 영역 조각`들로 이루어져있다고 생각하면 된다.\n",
    "\n",
    "- 은닉 노드 `p가 너무 크면 과잉적합`, `너무 작으면 과소적합`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. `단순화한 다층 퍼셉트론 학습 과정`\n",
    "![learn](./pic/learn.png)\n",
    "\n",
    "- `오차 계산`에서 `손실함수`를 사용한다. (with. `gradient`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cf. 신경망 경험적 개발\n",
    "![expre](./pic/expre.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
