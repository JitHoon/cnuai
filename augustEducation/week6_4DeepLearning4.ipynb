{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. DNN (깊은 신경망), Deep Learning (심층학습)\n",
    "- `다층 퍼셉트론에 은닉층`을 여러 개 `추가`하면 `깊은 신경`망이 됨\n",
    "- 은닉층이 4개 이상이면 깊은 다층 퍼셉트론 = 깊은 신경망 이라고 봄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-1) 현대 기계학습 특징\n",
    "#### `representation learning` (`표현 학습`)\n",
    "- 가공하지 않은 데이터라도 자동적으로 데이터로부터 특징을 추출할 수 있음\n",
    "- `end-to end 학습` (`종단 간 학습`) 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-2) `CNN (Convolution neural network)` \n",
    ": 컨볼루션(합성곱) 신경망\n",
    "\n",
    "#### `CONV (컨볼루션층)`\n",
    "- `선형함수인 컨볼루션`과 비선형 함수인 활성함수(ReLU)의 조합\n",
    "- 컨볼루션층을 잘 조합하여 CNN의 전체 구조를 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-3) `DMLP와 CNN의 비교`\n",
    "\n",
    "![DMNN](./pic/DMNN.png) \n",
    "\n",
    "#### `DMLP`\n",
    "- 완전 연결 구조로 높은 복잡도를 가짐\n",
    "- 학습이 매우 느리고 과잉적합 우려\n",
    "\n",
    "#### `CNN`\n",
    "- 희소한 부분에서 데이터를 추출하고 연산하는 `컨볼루션 연산`과 `가중치 공유`를 이용한 `부분연결 (희소 연결)`을 통해 구조의 복잡도를 낮춤\n",
    "- `필드(커널)를 옮겨가면서 컨볼루션 연산을 진행하고 각 커널에 대응되는 특증을 추출한다.`\n",
    "- `특징맵` : 각 층의 입출력의 특징형상 유지\n",
    "- `POOL 층` : `추출된 특징을 요약`하고 강화\n",
    "- `병렬 구조` : 각 노드는 독립적으로 계산 가능\n",
    "- `분산 구조` : 깊은 층을 거치면서 전체에 영향을 미침\n",
    "- 격자 구조 (영상, 음성 등)를 갖는 데이터에 적합\n",
    "- receptive field (수용장) 가 인간 시각과 유사\n",
    "- `가변 크기`의 데이터를 다룰 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-4) `convolution 컨볼루션 (합성곱) 연산`\n",
    "\n",
    " - 전체 대 전체를 연산하는 내적과 다른 `전체 대 부분을 연산하는 선형 연산`\n",
    "\n",
    "- 디지털신호처리!\n",
    "\n",
    "![convolution](./pic/convolution.png)\n",
    "\n",
    "- 아래와같이 컨볼루션 연산 후 원래 입력 차원보다 줄어들기 때문에 `padding`을 사용하여 차원을 보존시킴\n",
    "\n",
    "- 주로 zero padding을 사용\n",
    "\n",
    "![convolution2](./pic/convolution2.png)\n",
    "![convolution3](./pic/convolution3.png)\n",
    "\n",
    "- `filter = 관점 = 학습을 통해 결정되는 weight sharing`\n",
    "\n",
    "- filter에 bias값을 추가시키기도 함\n",
    "\n",
    "- 한 데이터에 다양한 filter를 적용시키는 것을 `다중 특징 맵 추출`이라고 한다.\n",
    "\n",
    "![buildingblock](./pic/buildingblock.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-5) `CNN 사례연구들`\n",
    "### 4-5-1) `AlexNet`\n",
    "#### `구조`\n",
    "\n",
    "![AlexNet](./pic/AlexNet.png)\n",
    "\n",
    "- `컨볼루션층` 5개와 `fully connected` (FC) 층 3개\n",
    "\n",
    "- 8개의 층에 각 노드 배치\n",
    "\n",
    "- `컨볼루션층 매개변수 200만개` < `FC층 매개변수 6500만개` (FC층이 30배, 향후 CNN은 FC층의 매개변수를 줄이는 방향으로 발전)\n",
    "\n",
    "- 2개 GPU로 분할하여 학습 수행\n",
    "\n",
    "- 컨볼루션층을 큰 폭으로 다운 샘플링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-5-2) `VGGNet`\n",
    "\n",
    "![VGG](./pic/VGG.png)\n",
    "\n",
    "- 컨볼루션층 8~16개 AlexNet에 비해 2~3배 깊어짐\n",
    "\n",
    "- VGG16 : CONV 13 + FC 3\n",
    "\n",
    "- `3*3의 작은 커널을 사용` (더 깊은 신경망)\n",
    "\n",
    "- 하지만 완전 연결층에서 `수많은 매개변수`를 가져 `과잉접합의 원인`이됨. 이를 GoogLeNet에서 `전역 평균 풀링`으로 매개변수 수를 줄여 해결\n",
    "\n",
    "#### `작은 커널이 좋은 이유`\n",
    "![VGGk](./pic/VGGk.png)\n",
    "- 큰 크기의 커널을 작은 크기 커널로 분해가능하다. = `매개변수의 수는 줄어들면서 신경망은 깊어지는 효과`를 볼 수 있다.\n",
    "\n",
    "- 참고 : 1*1 커널 : 차원 통합, 차원 축소 효과(연산 감소) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-5-3) `GoogLeNet`\n",
    "\n",
    "![GoogLeNet](./pic/GoogLeNet.png) \n",
    "\n",
    "- 기존 컨볼루션 연산을 MLPConv(마이크로 컨볼루션) 연산으로 대체 (`커널 대신 비선형 함수를 활성함수로 포함하는 MLP를 사용`하여 특징 추출 유리함)\n",
    "\n",
    "- 이후 더나아가 MLPConv 대신 9개의 `인셉션 모듈` 사용 (수용장의 `다양한 특징을 추출`하기 위해 NIN(network in network) 구조를 확장하여 `복수의 병렬적인 컨볼루션 층`을 가짐)\n",
    "\n",
    "- 매개변수가 있는 층 22개, 없는 층 (풀링) 5개로 총 27개 층 존재 (완전 연결층은 1개에 불과)\n",
    "\n",
    "- 신경망의 `미소 신경망`이 주어진 수용장의 특징을 추상화 시도\n",
    "\n",
    "![NIN](./pic/NIN.png)\n",
    "\n",
    "- `전역 평균 풀링`을 사용하므로 `매개변수 수가 0개`임 (최근에는 특징맵을 거의 1ㄷ1로 대응되도록 만든다.)\n",
    "\n",
    "- 1개있는 완전 연결층에 1백만개의 매개변수를 가지며, VGGNet의 완전 연층에 비하면 1%에 불과함\n",
    "\n",
    "- `보조 분류기`를 사용하여 경사 소멸 문제 완화 (학습땜나 도움, 추론 시 제거)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-5-4) `ResNet`\n",
    "- `잔류 (잔차)학습 residual learning` : 이전 CNN에서는 입력 x가 단순히 H(x)로의 출력으로 변한다는 사실만 학습했지만 `잔류 학습에서는 입력 x가 출력으로 F(x) + x 즉 입력대비 얼마나 변했다라는 기준으로 학습`하기 시작함, 이를 통해 과잉적합이라는 성능 저하를 피함과 동시에 층 수를 대폭 늘렸다.\n",
    "\n",
    "- 전역 평균 풀링을 사용하여 FC층을 제거하였다.\n",
    "\n",
    "- batch nomalization을 적용하여 드롭아웃 적용이 불필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![resnet](./pic/resnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- F(x) + x (잔차 + 입력)에 `타우를 적용`하여 `지름길`을 만들어둚. (깊은 신경망도 최적화가 가능, 경사 소멸 문제 해결(아래 그림 참고))\n",
    "\n",
    "![residual](./pic/residual.png)\n",
    "\n",
    "- residual term 에서 자기자신 1을 더해야하기 때문에 해당 term이 0이 되면서 gradient banish 문제를 해결할 수 있게됨 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pic/.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
