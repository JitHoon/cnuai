{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 기계학습과 수학\n",
    "- ![ML_concept](./pic/ML_concept.png)\n",
    "- 수학은 `목적함수`(성능 지표)를 정의하고, 목적함수의 `최저점`을 찾아주는 `최적화 이론`을 제공한다.\n",
    "- 수학은 목적함수뿐만 아니라 알고리즘, 데이터(수치적 형태, 벡터 등) 모두 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 2. 선형대수 (`Linear Algebra`) \n",
    "- : `연립방정식을 잘 풀기 위한 도구`\n",
    "## 2-1) 벡터(vector)와 행렬(matrix)\n",
    "- vector = 데이터의 종류(실수, 정수 등)와 크기 표현\n",
    "- `샘플` = `특징 벡터`(`Feature Vector`)\n",
    "- matrix = 여러개의 vector를 담음 (`vector로 인해서 형성되는 공간`) (`n행` : `n번째 데이터` (3번 Iris), `n열` : `데이터의 n번째 특징` (Iris의 꽃잎의 길이))\n",
    "- 행렬을 이용하면 `방정식을 간결하게 표현` 가능\n",
    "- ex) 전치 행렬, 정방행렬, 대각행렬, 단위행렬, 대칭행렬\n",
    "- 참고 : 전치 행렬 : 연산 순서를 바꿔줘야하는 경우 식을 간단하게 표현하기 위해 사용\n",
    "- `행렬 곱셈` = `벡터의 내적 연산` : `vector로 인해서 형성되는 공간을 변환시키는 것 (4차원 -> 2차원 등 새로운 공간으로의 이동)` (교환 법칙은 성립하지 않음, 분배, 결합 법칙은 성립 (AB)C = A(BC))\n",
    "-  `벡터의 내적` (곱셈과 합의 연산) : `방향`의 `유사도(similarity)`를 확인하는 연산 (같은 방향이면 양수, 반대 방향이면 음수, 90도면 0)\n",
    "- `Tensor` : `3차원 이상의 구조를 가진 숫자 배열(array)` (0차 : scalar, 1차 : vector, 2차 : matrix)\n",
    "- ![tensor](./pic/tensor.png)\n",
    "- `norm`: 벡터와 행렬의 `거리(크기)를 측정`할 때, 하강 기울기의 `규제`(벡터의 이동을 자유롭게 하지 못하게 boundary를 치는 것) 경우 (p차 norm, 최대 norm, Frobenius norm(행렬의 크기))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2) `퍼셉트론`(perceptron)\n",
    "- : 분류기(classifier) 모델) : 물리적 의미 : `데이터 x를 기준 w에 맞게 결정 직선, 평면으로 필터링하겠다.` \n",
    "- 방법 : 입력(x)과 기준(w)을 `내적`하여 scalar값을 구하고 그 값이 `활성(activation) 함수`의 `기준(threshold)`보다 높으면 1 낮으면 -1을 출력한다. \n",
    "- 이때 그 `기준을 나누는` 직선과 평면을 `결정 직선(2차원), 결정 평면(3차원), 결정 초평면(4차원)`이라고 부른다.\n",
    "- 여러개의 퍼셉트론 출력의 경우, 가중치(기준 w) 벡터와 입력(x)의 내적으로 표현(유사도 계산)된 출력은 벡터 o로 표현된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![perceptron1](./pic/perceptron1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![perceptron2](./pic/perceptron2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 퍼셉트론의 학습 : `훈련`, `추론`\n",
    "- `훈련` : `훈련집합`의 샘플에 가장 잘 만족하는 `w를 찾아내는 작업`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![training](./pic/training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `추론` : `학습을 마친` 알고리즘을 `현장의 새로운 데이터에 맞게 적용`하는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![inferring](./pic/inferring.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-3) Linear Combination, Vector Space\n",
    "- `Vector Space` : `Basis`(기저) Vector의 `Linear Combination`으로 만들어지는 공간"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-4) Matrix Invesion\n",
    "- 물리적 의미 : 원래 있었던 공간으로 `다시 되돌려주는 역할`을 하는 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 역행렬을 활용한 `방정식 표현` : Ax = b\n",
    "- `선형 방정식일 경우의 해` : `불능` = 해 없음, `부정` = 다수의 해 존재, `유일한 존재` = 역행렬을 이용하여 해를 구할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (노트필기 참고)\n",
    "- 역행렬 존재 조건\n",
    "- determinant\n",
    "- decomposition\n",
    "- eigenvalue, eigenvector\n",
    "- eigen-decomposition (고유 분해)\n",
    "- singular value decomposition (SVD, 특이값 분해) : 정사걱행렬이 아닌 행렬의 역행렬 계산 시 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 3. 확률과 통계\n",
    "- 데이터에 들어오는 `불확실성(uncertainty)을 측정`하기위해 사용하는 도구"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1) 확률 기초\n",
    "- `random variable` : 확률 변수 x (윷)\n",
    "- `random variable`의 `domain` : 확률 변수의 정의역 {도, 개, 걸, 윷, 모}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `probability distribution` : 확률 분포\n",
    "- `내가 처리하고자하는 RV이 어떤 형태로 발현이 되는지에대한 규칙을 설명하는 방법`\n",
    "- 1. `확률 질량 함수` (PMF, probabillity `mass` function : 이산 확률 변수 (`discrete`)\n",
    "- 2. `확률 밀도 함수` (PDF, probabillity `density` function) : 연속 확률 변수 (`continous`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `random vector` : 확률 벡터\n",
    "- RV를 요소로 가지는 벡터 : 벡터 x = (꽃받핌 길이, 꽃받침 너비, 꽃잎 길이, 꽃잎 너비)^T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1-1) `Bayes's rule` (베이즈 정리) \n",
    "- : `분류 문제`에서 `사후 확률`(결과를 먼저 알고 원인을 모르는 상태에서 `결과를 일으킨 원인의 확률을 찾는 과정`) `을 간접적으로 추정하는 도구`\n",
    "- ex) 하얀 공이 나왔다는 사실만 알고 어느 병에서 나왔는지 모르는데, 어느 병인지 추정하라."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bayesrule_ex](./pic/bayesrule_ex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `사후`(posteriori, 결과가 일어난 후 원인을 찾는, 위의 경우 하양 = x = 결과) 확률 = `우도`(likelihood, 원인에대한 실제 결과들 = 관찰된 값 = 원하는 것만 관찰하여 그렇게 보일 것이다 라는  결과) 확률 * `사전` (prior, 원인이 발생할 확률, 위의 경우 몇번째 병 = y = 원인) 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bayes_mean](./pic/bayes_mean.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1-2) `maximum likelihood` (최대 우도) \n",
    "- : 데이터 x가 세타에 의해서 발현되는데 눈에 보이지 않는 세타를 데이터 x를 보고 likelihood를 통해서 세타를 설명하는 방법\n",
    "- 아래 일반화식의 경우 `단조 증가 함수`이므로 로그 함수를 이용해도 `최댓값이 변하지 않는다.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![maximum_likelihood](./pic/maximum_likelihood.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1-3) 데이터의 `요약 정보` \n",
    "- : `mean`(평균), `variance` (분산)\n",
    "- 1. `mean`(평균) 벡터 : 치우침 정도\n",
    "- 2. `covariance matrix` (공분산 헹렬) : RV의 상관 정도\n",
    "- 두 지표 모두 `데이터간의 퍼짐이나 상관 관계를 알 수 있는` 지표이다. \n",
    "- 두 지표로 다차원 가우시안 분포를 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1-4) 유용한 확률 분포\n",
    "- `베르누이 분포`\n",
    "- `이항 분포`\n",
    "- `가우시안 분포`\n",
    "- 지수 분포\n",
    "- 라플라스 분포\n",
    "- 디랙 분포\n",
    "- `혼합 분포` : 서로 다른 분포를 사용하여 데이터를 설명하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1-5) 확률 분포와 관련된 유용한 함수들\n",
    "- `로지스틱 시그모이드 함수`\n",
    "- `소프트플러스 함수`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1-6) 변수 변환\n",
    "- 기존 확률 변수를 새로운 확률 변수로 바꾸는 것 (데이터를 임의로 생성하고 가공할 때 이용)\n",
    "- 정의역과 치역을 바꾸어서 새로운 함수를 만들 수 있음 g(y) = ~y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 정보이론\n",
    "- 확률 분포 추정 후 그 분포 간의 `유사성을 정량화`\n",
    "- 즉 `불확실정을 정량화`하여 기계학습에 적용\n",
    "- ex) 엔트로피, 교차 엔트로피, KL 발산, 상대 엔트로피\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-1) 정보이론의 기본 원리\n",
    "### 4-1-1) 사건과 정보량\n",
    "- 사건이 `발생할 확률이 작을수록`(잘 일어나지 않는 사건) `정보량(informative)이 많음` = `불확실성이 많음`\n",
    "#### 1. `자기 정보` (self information)\n",
    "- 한 사건(`확률 P(e)`)의 한 정보량"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![self_information](./pic/self_information.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. `엔트로피` (entropy) - `각 확률간의 분포 유사도`를 따질 때만 사용\n",
    "- `모든` 자기 정보(한 정보량)의 기대 값 표현\n",
    "- `확률변수 x의 불확실성`을 나타냄 (= `해당 사건의 전체 정보량`)\n",
    "- 데이터 전달에 필요한 `최대 정보량`(비트)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. `교차 엔트로피` (cross entropy)와 `KL 발산`(divergence)\n",
    "- `두 확률분포 P와 Q 사이의 교차 엔트로피`\n",
    "- `정답과 예측값을 비교하는 손실함수`로 많이 사용됨\n",
    "- `정답과 예측값을 근사화` = `손실함수 최소` = `KL 발산의 최소화`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cross_entropy_KL](./pic/cross_entropy_KL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `KL 발산`의 경우 두 `확률분포 사이의 거리를 계산`할 때 주로 사용\n",
    "- `KL 발산`을 줄인다는 의미는 `가지고있는 데이터 P`와 `추정한 데이터 분포 Q`간의 `차이를 최소화`하는 것과 같다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
